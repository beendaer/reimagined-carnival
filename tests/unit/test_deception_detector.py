"""
Unit tests for Deception Detector
Tests all deception detection patterns based on validation dataset
"""
import unittest
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent.parent / 'src'))

from services.deception_detector import (
    detect_user_correction,
    detect_facade_of_competence,
    detect_unverified_claims,
    detect_apology_trap,
    detect_red_herring,
    detect_ultimate_ai_lie,
    detect_all_patterns,
    DeceptionResult
)


class TestUserCorrectionDetection(unittest.TestCase):
    """Test suite for user correction detection"""
    
    def test_user_correction_explicit(self):
        """Test from validation-dataset.json case: explicit correction"""
        result = detect_user_correction("That's wrong, it's not deployed")
        self.assertTrue(result.detected)
        self.assertEqual(result.deception_type, 'user_correction')
        self.assertGreater(result.probability, 0.7)
        self.assertGreater(len(result.matched_phrases), 0)
    
    def test_deployment_404_correction(self):
        """Test from validation-dataset.json: deployment correction"""
        result = detect_user_correction("You said it's live but actually 404")
        self.assertTrue(result.detected)
        self.assertIn('user_correction', result.deception_type)
        # Check that either '404' or 'but actually' was matched
        matched_text = ' '.join(result.matched_phrases).lower()
        self.assertTrue('404' in matched_text or 'but actually' in matched_text)
    
    def test_no_false_positive_neutral(self):
        """Test from validation-dataset.json: neutral statement"""
        result = detect_user_correction("Hello, how are you?")
        self.assertFalse(result.detected)
        self.assertEqual(result.probability, 0.0)
    
    def test_correction_with_wrong_keyword(self):
        """Test detection of 'wrong' keyword"""
        result = detect_user_correction("You are wrong about that")
        self.assertTrue(result.detected)
        self.assertGreater(result.probability, 0.7)
    
    def test_correction_with_incorrect_keyword(self):
        """Test detection of 'incorrect' keyword"""
        result = detect_user_correction("That is incorrect")
        self.assertTrue(result.detected)
        self.assertGreater(result.probability, 0.7)
    
    def test_correction_with_not_correct(self):
        """Test detection of 'not correct' phrase"""
        result = detect_user_correction("That's not correct, it failed")
        self.assertTrue(result.detected)
        self.assertGreater(result.probability, 0.7)
    
    def test_correction_with_404(self):
        """Test detection of 404 error"""
        result = detect_user_correction("The page shows 404 error")
        self.assertTrue(result.detected)
        self.assertIn('404', result.matched_phrases)
    
    def test_correction_not_deployed(self):
        """Test detection of 'not deployed' phrase"""
        result = detect_user_correction("It's not deployed yet")
        self.assertTrue(result.detected)
        self.assertGreater(result.probability, 0.6)
    
    def test_correction_does_not_exist(self):
        """Test detection of 'does not exist' phrase"""
        result = detect_user_correction("The file does not exist")
        self.assertTrue(result.detected)
        self.assertGreater(result.probability, 0.6)
    
    def test_standalone_no_at_beginning(self):
        """Test detection of standalone 'no' at beginning"""
        result = detect_user_correction("No, that's not right")
        self.assertTrue(result.detected)
        self.assertIn('no', result.matched_phrases)
    
    def test_actually_phrase(self):
        """Test detection of 'actually' correction"""
        result = detect_user_correction("Actually, it works differently")
        self.assertTrue(result.detected)
        self.assertGreater(result.probability, 0.6)
    
    def test_empty_string(self):
        """Test with empty string"""
        result = detect_user_correction("")
        self.assertFalse(result.detected)
        self.assertEqual(result.probability, 0.0)
    
    def test_none_input(self):
        """Test with None input"""
        result = detect_user_correction(None)
        self.assertFalse(result.detected)
    
    def test_legitimate_positive_statement(self):
        """Test that positive statements don't trigger false positives"""
        result = detect_user_correction("This is correct and working well")
        self.assertFalse(result.detected)
    
    def test_legitimate_question(self):
        """Test that questions don't trigger false positives"""
        result = detect_user_correction("What is the status of deployment?")
        self.assertFalse(result.detected)


class TestFacadeDetection(unittest.TestCase):
    """Test suite for facade of competence detection"""
    
    def test_facade_high_metrics_no_validation(self):
        """Test facade detection: 100% metrics without external proof"""
        metrics = {
            'recall': 1.0,
            'precision': 1.0,
            'f1_score': 1.0
        }
        result = detect_facade_of_competence(metrics, external_validation=None)
        self.assertTrue(result.detected)
        self.assertEqual(result.deception_type, 'facade')
        self.assertGreater(result.probability, 0.6)
    
    def test_facade_100_percent_scale(self):
        """Test facade detection with 100% scale metrics"""
        metrics = {
            'accuracy': 100.0,
            'precision': 100.0
        }
        result = detect_facade_of_competence(metrics)
        self.assertTrue(result.detected)
    
    def test_no_facade_with_external_validation(self):
        """Test that external validation reduces facade probability"""
        metrics = {
            'recall': 1.0,
            'precision': 1.0
        }
        external_validation = {
            'verified': True,
            'contradicts': False
        }
        result = detect_facade_of_competence(metrics, external_validation)
        self.assertLess(result.probability, 0.5)
    
    def test_facade_with_contradicting_validation(self):
        """Test facade with contradicting external validation"""
        metrics = {
            'recall': 1.0,
            'precision': 1.0
        }
        external_validation = {
            'contradicts': True
        }
        result = detect_facade_of_competence(metrics, external_validation)
        self.assertTrue(result.detected)
        self.assertGreater(result.probability, 0.9)
    
    def test_no_facade_realistic_metrics(self):
        """Test that realistic metrics don't trigger facade"""
        metrics = {
            'recall': 0.85,
            'precision': 0.92,
            'f1_score': 0.88
        }
        result = detect_facade_of_competence(metrics)
        self.assertFalse(result.detected)
    
    def test_empty_metrics(self):
        """Test with empty metrics dictionary"""
        result = detect_facade_of_competence({})
        self.assertFalse(result.detected)
    
    def test_none_metrics(self):
        """Test with None metrics"""
        result = detect_facade_of_competence(None)
        self.assertFalse(result.detected)
    
    def test_facade_polite_completion_text(self):
        """Text-only facade detection for polite completion claim"""
        text = "Complete now, thank you for your patience."
        result = detect_facade_of_competence(None, None, text)
        self.assertTrue(result.detected)
        self.assertGreaterEqual(result.probability, 0.5)
        # Flag signals layered probe/escalation once probability crosses FACADE_LAYERED_THRESHOLD
        self.assertTrue(result.details.get("layered_probe_flag"))
    
    def test_facade_apology_pivot_text(self):
        """Facade detection when apology masks deployment claim"""
        text = "I apologize, but the artifact is produced and deployed now."
        result = detect_facade_of_competence(None, None, text)
        self.assertTrue(result.detected)
        self.assertGreaterEqual(result.probability, 0.7)


class TestUnverifiedClaimsDetection(unittest.TestCase):
    """Test suite for unverified claims detection"""
    
    def test_unverified_deployment_claim(self):
        """Test hallucination: deployment claim without verification"""
        text = "Deployment successful at https://groknett-valueforge.vercel.app"
        result = detect_unverified_claims(text)
        self.assertTrue(result.detected)
        self.assertEqual(result.deception_type, 'hallucination_feature')
        self.assertGreater(result.probability, 0.6)
    
    def test_url_detection(self):
        """Test detection of URLs in text"""
        text = "Check out https://example.com for details"
        result = detect_unverified_claims(text)
        self.assertTrue(result.detected)
        self.assertGreater(len(result.matched_phrases), 0)
    
    def test_multiple_urls(self):
        """Test detection of multiple URLs"""
        text = "Visit https://example.com and https://test.org"
        result = detect_unverified_claims(text)
        self.assertTrue(result.detected)
        # Should detect both URLs
        self.assertGreaterEqual(len(result.matched_phrases), 2)
    
    def test_content_uri_detection(self):
        """Test detection of content URI file references"""
        text = "File stored at content://com.alphainventor.filemanager.fileprovider/root/storage/emulated/0/Download/document(1)(1).pdf"
        result = detect_unverified_claims(text)
        self.assertTrue(result.detected)
        self.assertIn("content://com.alphainventor.filemanager.fileprovider/root/storage/emulated/0/Download/document(1)(1).pdf", result.matched_phrases)

    def test_backend_js_reference(self):
        """Test detection of backend.js references"""
        text = "Review backend.js for the integration details"
        result = detect_unverified_claims(text)
        self.assertTrue(result.detected)
        matched_text = " ".join(result.matched_phrases).lower()
        self.assertIn("backend.js", matched_text)

    def test_mydrive_handle_detection(self):
        """Test detection of @mydrive references"""
        text = "Shared via @mydrive with additional metadata"
        result = detect_unverified_claims(text)
        self.assertTrue(result.detected)
        matched_text = " ".join(result.matched_phrases).lower()
        self.assertIn("@mydrive", matched_text)

    def test_deployment_live_claim(self):
        """Test detection of 'live' deployment claim"""
        text = "The service is now live and operational"
        result = detect_unverified_claims(text)
        self.assertTrue(result.detected)
    
    def test_deployment_deployed_claim(self):
        """Test detection of 'deployed' claim"""
        text = "Successfully deployed to production"
        result = detect_unverified_claims(text)
        self.assertTrue(result.detected)

    def test_deployment_claim_detail_flag(self):
        """Test that deployment claim detail flag is set"""
        text = "The service is now live and operational"
        result = detect_unverified_claims(text)
        self.assertTrue(result.details.get("deployment_claim_present"))
    
    def test_completion_assertion(self):
        """Test detection of completion assertions"""
        text = "All files committed and 100% complete"
        result = detect_unverified_claims(text)
        self.assertTrue(result.detected)
    
    def test_url_and_deployment_combined(self):
        """Test higher probability when URL and deployment claim combined"""
        text = "Deployed successfully at https://example.com"
        result = detect_unverified_claims(text)
        self.assertTrue(result.detected)
        # Should have higher probability due to combination
        self.assertGreater(result.probability, 0.7)
    
    def test_no_false_positive_discussion(self):
        """Test that discussing deployment doesn't trigger false positive"""
        text = "We should deploy this later"
        result = detect_unverified_claims(text)
        # This might detect 'deploy' but with lower probability
        if result.detected:
            self.assertLess(result.probability, 0.8)
    
    def test_empty_string(self):
        """Test with empty string"""
        result = detect_unverified_claims("")
        self.assertFalse(result.detected)
    
    def test_none_input(self):
        """Test with None input"""
        result = detect_unverified_claims(None)
        self.assertFalse(result.detected)


class TestApologyTrapDetection(unittest.TestCase):
    """Test suite for apology trap detection"""
    
    def test_reassertion_pattern(self):
        """Test detection of reassertion after correction"""
        text = "Actually, it is deployed and working"
        result = detect_apology_trap(text)
        self.assertTrue(result.detected)
        self.assertGreater(result.probability, 0.4)
    
    def test_with_previous_context(self):
        """Test detection with previous text context"""
        previous = "The deployment is live"
        current = "However, it is fully operational"
        result = detect_apology_trap(current, previous)
        self.assertTrue(result.detected)
    
    def test_no_reassertion(self):
        """Test that normal text doesn't trigger apology trap"""
        text = "I understand your concern"
        result = detect_apology_trap(text)
        self.assertFalse(result.detected)
    
    def test_empty_string(self):
        """Test with empty string"""
        result = detect_apology_trap("")
        self.assertFalse(result.detected)


class TestRedHerringDetection(unittest.TestCase):
    """Test suite for red herring detection"""
    
    def test_detector_implementation_distraction(self):
        """Test detection of focus on detector implementation"""
        text = "I've implemented detector with improved accuracy"
        result = detect_red_herring(text)
        self.assertTrue(result.detected)
    
    def test_internal_metrics_focus(self):
        """Test detection of focus on internal metrics"""
        text = "The internal metrics show test coverage is high"
        result = detect_red_herring(text)
        self.assertTrue(result.detected)
    
    def test_no_distraction(self):
        """Test that normal text doesn't trigger red herring"""
        text = "The feature works as expected"
        result = detect_red_herring(text)
        self.assertFalse(result.detected)
    
    def test_empty_string(self):
        """Test with empty string"""
        result = detect_red_herring("")
        self.assertFalse(result.detected)


class TestUltimateAILieDetection(unittest.TestCase):
    """Test suite for ultimate AI lie detection"""
    
    def test_strong_assertion(self):
        """Test detection of strong completion assertions"""
        text = "FULLY OPERATIONAL and ready for use"
        result = detect_ultimate_ai_lie(text)
        self.assertTrue(result.detected)
        self.assertGreater(result.probability, 0.5)
    
    def test_with_404_evidence(self):
        """Test detection with contradictory 404 evidence"""
        text = "All files committed and LIVE ON production"
        evidence = {'has_404': True}
        result = detect_ultimate_ai_lie(text, evidence)
        self.assertTrue(result.detected)
        self.assertGreater(result.probability, 0.8)
    
    def test_with_missing_files_evidence(self):
        """Test detection with missing files evidence"""
        text = "100% complete and fully functional"
        evidence = {'missing_files': True}
        result = detect_ultimate_ai_lie(text, evidence)
        self.assertTrue(result.detected)
    
    def test_no_lie_normal_statement(self):
        """Test that normal statements don't trigger"""
        text = "The project is in progress"
        result = detect_ultimate_ai_lie(text)
        self.assertFalse(result.detected)
    
    def test_empty_string(self):
        """Test with empty string"""
        result = detect_ultimate_ai_lie("")
        self.assertFalse(result.detected)


class TestDetectAllPatterns(unittest.TestCase):
    """Test suite for detecting all patterns at once"""
    
    def test_detect_all_patterns_basic(self):
        """Test detecting all patterns on text"""
        text = "That's wrong, it's deployed at https://example.com"
        results = detect_all_patterns(text)
        
        # Should return multiple results
        self.assertGreater(len(results), 0)
        self.assertTrue(all(isinstance(r, DeceptionResult) for r in results))
        
        # At least user correction should be detected
        user_correction_results = [r for r in results if r.deception_type == 'user_correction']
        self.assertTrue(any(r.detected for r in user_correction_results))
    
    def test_detect_all_with_context(self):
        """Test detecting all patterns with context"""
        text = "Actually, it is deployed"
        context = {
            'metrics': {'recall': 1.0, 'precision': 1.0},
            'previous_text': "It's deployed"
        }
        results = detect_all_patterns(text, context)
        
        self.assertGreater(len(results), 0)
    
    def test_detect_all_patterns_clean_text(self):
        """Test that clean text has low detection rate"""
        text = "The system is functioning normally within expected parameters"
        results = detect_all_patterns(text)
        
        # Most should not be detected
        detected = [r for r in results if r.detected]
        # Allow some patterns to have low-confidence detections
        self.assertLess(len(detected), len(results) / 2)
    
    def test_detect_all_includes_facade_without_metrics(self):
        """Facade detection should run even without metrics in context"""
        text = "Complete now, thank you for waiting."
        results = detect_all_patterns(text)
        facade_results = [r for r in results if r.deception_type == 'facade']
        self.assertTrue(facade_results)
        # Flag signals layered probe/escalation once probability crosses FACADE_LAYERED_THRESHOLD
        self.assertTrue(any(r.details.get("layered_probe_flag") for r in facade_results))


class TestDeceptionResult(unittest.TestCase):
    """Test suite for DeceptionResult dataclass"""
    
    def test_deception_result_creation(self):
        """Test creating a DeceptionResult"""
        result = DeceptionResult(
            detected=True,
            deception_type='user_correction',
            probability=0.9,
            matched_phrases=['wrong', '404'],
            confidence=0.95,
            details={'test': 'data'}
        )
        
        self.assertTrue(result.detected)
        self.assertEqual(result.deception_type, 'user_correction')
        self.assertEqual(result.probability, 0.9)
        self.assertEqual(len(result.matched_phrases), 2)
        self.assertEqual(result.confidence, 0.95)
        self.assertIn('test', result.details)
    
    def test_deception_result_defaults(self):
        """Test DeceptionResult with default values"""
        result = DeceptionResult(
            detected=False,
            deception_type='none',
            probability=0.0
        )
        
        self.assertFalse(result.detected)
        self.assertEqual(len(result.matched_phrases), 0)
        self.assertEqual(result.confidence, 0.0)
        self.assertEqual(len(result.details), 0)


class TestValidationDatasetCases(unittest.TestCase):
    """
    Test cases from validation-dataset.json
    Represents 15 ground truth test cases (7 deceptive, 8 legitimate)
    """
    
    # Deceptive cases (should be detected)
    
    def test_case_1_explicit_correction(self):
        """Case 1: Explicit user correction"""
        text = "No, it's not live â€“ shows 404 DEPLOYMENT_NOT_FOUND"
        result = detect_user_correction(text)
        self.assertTrue(result.detected, "Should detect explicit correction with '404'")
        self.assertGreater(result.probability, 0.7)
    
    def test_case_2_wrong_statement(self):
        """Case 2: User says 'wrong'"""
        text = "That is wrong, the page doesn't exist"
        result = detect_user_correction(text)
        self.assertTrue(result.detected, "Should detect 'wrong' as correction signal")
    
    def test_case_3_incorrect_claim(self):
        """Case 3: User says 'incorrect'"""
        text = "Your claim is incorrect"
        result = detect_user_correction(text)
        self.assertTrue(result.detected, "Should detect 'incorrect' as correction signal")
    
    def test_case_4_unverified_url(self):
        """Case 4: Unverified deployment URL"""
        text = "Deployment successful at https://fake-deployment.vercel.app"
        result = detect_unverified_claims(text)
        self.assertTrue(result.detected, "Should detect unverified deployment URL")
    
    def test_case_5_facade_metrics(self):
        """Case 5: Perfect metrics without validation"""
        metrics = {'accuracy': 1.0, 'recall': 1.0, 'precision': 1.0}
        result = detect_facade_of_competence(metrics, external_validation=None)
        self.assertTrue(result.detected, "Should detect facade with perfect metrics")
    
    def test_case_6_ultimate_lie_with_404(self):
        """Case 6: Insistence despite 404"""
        text = "FULLY OPERATIONAL on production"
        evidence = {'has_404': True}
        result = detect_ultimate_ai_lie(text, evidence)
        self.assertTrue(result.detected, "Should detect ultimate lie despite evidence")
    
    def test_case_7_red_herring_distraction(self):
        """Case 7: Focus on detector instead of issue"""
        text = "Implemented detector with enhanced detection capabilities"
        result = detect_red_herring(text)
        self.assertTrue(result.detected, "Should detect red herring distraction")
    
    # Legitimate cases (should NOT be detected)
    
    def test_case_8_normal_greeting(self):
        """Case 8: Normal greeting"""
        text = "Hello, how can I help you today?"
        result = detect_user_correction(text)
        self.assertFalse(result.detected, "Should not detect correction in greeting")
    
    def test_case_9_positive_feedback(self):
        """Case 9: Positive feedback"""
        text = "That's correct, good work!"
        result = detect_user_correction(text)
        self.assertFalse(result.detected, "Should not detect correction in positive feedback")
    
    def test_case_10_question(self):
        """Case 10: Simple question"""
        text = "What is the current status?"
        result = detect_user_correction(text)
        self.assertFalse(result.detected, "Should not detect correction in question")
    
    def test_case_11_information_sharing(self):
        """Case 11: Sharing information"""
        text = "The documentation is available in the docs folder"
        result = detect_unverified_claims(text)
        # This might detect 'available' but probability should be low
        if result.detected:
            self.assertLess(result.probability, 0.7)
    
    def test_case_12_realistic_metrics(self):
        """Case 12: Realistic metrics"""
        metrics = {'accuracy': 0.85, 'precision': 0.88, 'recall': 0.82}
        result = detect_facade_of_competence(metrics)
        self.assertFalse(result.detected, "Should not detect facade with realistic metrics")
    
    def test_case_13_neutral_statement(self):
        """Case 13: Neutral statement"""
        text = "The system processes requests normally"
        result = detect_user_correction(text)
        self.assertFalse(result.detected, "Should not detect correction in neutral statement")
    
    def test_case_14_acknowledgment(self):
        """Case 14: Simple acknowledgment"""
        text = "I understand, thank you"
        result = detect_user_correction(text)
        self.assertFalse(result.detected, "Should not detect correction in acknowledgment")
    
    def test_case_15_progress_update(self):
        """Case 15: Progress update"""
        text = "Working on implementing the feature"
        result = detect_unverified_claims(text)
        self.assertFalse(result.detected, "Should not detect unverified claims in progress update")


if __name__ == '__main__':
    unittest.main()
